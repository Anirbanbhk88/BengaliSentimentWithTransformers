# -*- coding: utf-8 -*-
"""BERT-FineTuning_2class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1psfknJv7eIQuQGoBbgbmtb2PouBHO_Gp
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import matplotlib.pyplot as plt
import random
import numpy as np
import torch.nn.functional as F
import pandas as pd



SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True

#! pip install sentencepiece

base_path = '/content/drive/MyDrive/Independent Study-NLP/Bengali_Sentiment-master/' #'IAS/SEMESTER 4/IS-NLP/Bengali_Sentiment-master/'
model_name = 'BERT-FineTuning'
classification_class = '2class_'
result_path = 'Results/training_results/'
report_path = 'Results/classification_report/'
Dataset = 'YouTube'#'ProthomAlo' #'BookReviews'



MODEL_TYPE = 'bert-base-multilingual-cased'
#tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)
#roberta = XLMRobertaModel.from_pretrained(MODEL_TYPE)

# If there's a GPU available...
if torch.cuda.is_available():    

    # Tell PyTorch to use the GPU.    
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

!pip install transformers

"""### Loading Dataset"""

df_train = pd.read_csv(base_path +'Dataset/'+Dataset + '/youtube_drama_train.csv')
df_test = pd.read_csv(base_path +'Dataset/'+Dataset + '/youtube_drama_test.csv')

df_train

"""### Tokenization for BERT and Formatting
To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index from the tokenizer vocabulary.

The tokenization must be performed by the BERT's own tokenizer
"""

from transformers import BertTokenizer

# Load the BERT tokenizer.
print('Loading BERT tokenizer...')
tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE, do_lower_case=True)

"""Let's apply the tokenizer to one sentence just to see the output.

"""

# Get the lists of sentences and their labels.
sentences = df_train.Data.values
labels = df_train.Sentiment.values
# Print the original sentence.
print(' Original: ', sentences[0])

# Print the sentence split into tokens.
print('Tokenized: ', tokenizer.tokenize(sentences[0]))

# Print the sentence mapped to token ids.
print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))

"""We are required to:

1. Add special tokens to the start and end of each sentence.
2. Pad & truncate all sentences to a single constant length.
3. Explicitly differentiate real tokens from padding tokens with the "attention mask".
"""

max_input_length = 0
total_df = pd.concat([df_train, df_test])
print(total_df.shape)
# For every sentence calculate the MAX Length
for sent in sentences:

    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.
    #print(sent)
    input_ids = tokenizer.encode(sent, add_special_tokens=True)

    # Update the maximum sentence length.
    max_input_length = max(max_input_length, len(input_ids))

print('Max sentence length: ', max_input_length)

max_input_length = 400 ##round the max length to 400
print(max_input_length)

def generate_encodings(sentences, labels):
  # Tokenize all of the sentences and map the tokens to thier word IDs.
  input_ids = []
  attention_masks = []

  # For every sentence...
  for sent in sentences:
      # `encode_plus` will:
      #   (1) Tokenize the sentence.
      #   (2) Prepend the `[CLS]` token to the start.
      #   (3) Append the `[SEP]` token to the end.
      #   (4) Map tokens to their IDs.
      #   (5) Pad or truncate the sentence to `max_length`
      #   (6) Create attention masks for [PAD] tokens.
      encoded_dict = tokenizer.encode_plus(
                          sent,                      # Sentence to encode.
                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                          max_length = max_input_length,           # Pad & truncate all sentences.
                          padding = 'max_length',
                          truncation = True,
                          return_attention_mask = True,   # Construct attn. masks.
                          return_tensors = 'pt',     # Return pytorch tensors.
                    )
      
      # Add the encoded sentence to the list.    
      input_ids.append(encoded_dict['input_ids'])
      
      # And its attention mask (simply differentiates padding from non-padding).
      attention_masks.append(encoded_dict['attention_mask'])

      

  # Convert the lists into tensors.
  input_ids = torch.cat(input_ids, dim=0)
  attention_masks = torch.cat(attention_masks, dim=0)
  labels = torch.tensor(labels)
  #encoded_labels = pd.get_dummies(labels)
  #labels = torch.tensor(encoded_labels.to_numpy())

  # Print sentence 0, now as a list of IDs.
  print('Original: ', sentences[0])
  print('Token IDs:', input_ids[0])
  return input_ids, attention_masks, labels

from torch.utils.data import TensorDataset, random_split

#generate token ids for training input sentences, attention mask, and labels as tensors using BERT encoding
input_ids, attention_masks, labels = generate_encodings(df_train.Data.values, df_train.Sentiment.values)


# Combine the training inputs into a TensorDataset.
dataset = TensorDataset(input_ids, attention_masks, labels)

# Create a 70-15-15 train-validation-test split.

# Calculate the number of samples to include in each set.
train_size = int(0.85 * len(dataset))
val_size = len(dataset) - train_size

# Divide the dataset by randomly selecting samples.
train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(SEED))


input_ids, attention_masks, labels = generate_encodings(df_test.Data.values, df_test.Sentiment.values)
# Combine the training inputs into a TensorDataset.
test_dataset = TensorDataset(input_ids, attention_masks, labels)

print('{:>5,} training samples'.format(train_size))
print('{:>5,} validation samples'.format(val_size))
print('{:>5,} test samples'.format(len(test_dataset)))

labels[:10]

test_dataset[0]

"""We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory"""

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

# The DataLoader needs to know our batch size for training, so we specify it 
# here. For fine-tuning BERT on a specific task, the authors recommend a batch 
# size of 16 or 32.
batch_size = 32

# Create the DataLoaders for our training and validation sets.
# We'll take training samples in random order. 
train_dataloader = DataLoader(
            train_dataset,  # The training samples.
            sampler = RandomSampler(train_dataset), # Select batches randomly
            batch_size = batch_size # Trains with this batch size.
        )

# For validation the order doesn't matter, so we'll just read them sequentially.
validation_dataloader = DataLoader(
            val_dataset, # The validation samples.
            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.
            batch_size = batch_size # Evaluate with this batch size.
        )

test_dataloader = DataLoader(
            test_dataset, # The validation samples.
            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.
            batch_size = batch_size # Evaluate with this batch size.
        )

"""### Classification Model: BertForSequenceClassification

We'll be using BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.
"""

from transformers import BertForSequenceClassification, AdamW, BertConfig

# Load BertForSequenceClassification, the pretrained BERT model with a single 
# linear classification layer on top. 
model = BertForSequenceClassification.from_pretrained(
    MODEL_TYPE, # Use the 12-layer BERT model. Here the model used is 'bert-base-multilingual-cased'
    num_labels = 2, # The number of output labels--2 for binary classification.
                    # You can increase this for multi-class tasks.   
    output_attentions = False, # Whether the model returns attentions weights.
    output_hidden_states = False, # Whether the model returns all hidden-states.
)

# Tell pytorch to run this model on the GPU.
model = model.to(device)

# Get all of the model's parameters as a list of tuples.
params = list(model.named_parameters())

print('The BERT model has {:} different named parameters.\n'.format(len(params)))

print('==== Embedding Layer ====\n')

for p in params[0:5]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

print('\n==== First Transformer Layer ====\n')

for p in params[5:21]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

print('\n==== Output Layer ====\n')

for p in params[-4:]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

"""### Optimizer & Learning Rate Scheduler

For the purposes of fine-tuning, the authors recommend choosing from the following values:

* Batch size: 16, 32
* Learning rate (Adam): 5e-5, 3e-5, 2e-5
* Number of epochs: 2, 3, 4

We chose:

* Batch size: 32 (set when creating our DataLoaders)
* Learning rate: 2e-5
* Epochs: 4 (we'll see that this is probably too many...)
"""

# Note: AdamW is a class from the huggingface library (as opposed to pytorch) 
# 'W' stands for 'Weight Decay fix"
learning_rate = 2e-5
optimizer = AdamW(model.parameters(),
                  lr = learning_rate, # args.learning_rate - default is 5e-5, 3e-5, 2e-5
                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.
                )

"""Setup linear scheduler for warm up of the classification layers during fine tuning. Due to linear scheduler the LR gradually increases leading to untrained classification layers to train more therby bringing it upto the level of the pretrained BERT layers. And then the LR reduces gradually"""

from transformers import get_linear_schedule_with_warmup

# Number of training epochs. The BERT authors recommend between 2 and 4. 
# We chose to run for 4, but we'll see later that this may be over-fitting the
# training data.
epochs = 4

# Total number of training steps is [number of batches] x [number of epochs]. 
# (Note that this is not the same as the number of training samples).
total_steps = len(train_dataloader) * epochs

# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(optimizer, 
                                            num_warmup_steps = 220, # Default value in run_glue.py
                                            num_training_steps = total_steps)

print(len(train_dataloader))
print(total_steps)

# lrs = []
# for i in range(total_steps):
#     optimizer.step()
#     lrs.append(optimizer.param_groups[0]["lr"])
#     scheduler.step()
    
# plt.plot(lrs)
# plt.show()

"""Mapping the Sentiment Labels to indices S.T -----------> 0: Negative, 1: Positive"""

import numpy as np

# Function to calculate the accuracy of our predictions vs labels
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    #print(f"predicted labels: {pred_flat}, length:{len(preds)}")
    labels_flat = labels.flatten()
    #print(f"true labels: {labels_flat}")
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

import time
import datetime

def format_time(elapsed):
    '''
    Takes a time in seconds and returns a string hh:mm:ss
    '''
    # Round to the nearest second.
    elapsed_rounded = int(round((elapsed)))
    
    # Format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))

"""Start the training"""

def train():
  # ========================================
    #               Training
    # ========================================
    
    # Perform one full pass over the training set.

    print("")
    
    print('Training...')

    # Measure how long the training epoch takes.
    t0 = time.time()

    # Reset the total loss for this epoch.
    total_train_loss = 0
    total_train_accuracy = 0

    model.train()

    # For each batch of training data...
    for step, batch in enumerate(train_dataloader):

        # # Progress update every 40 batches.
        # if step % 40 == 0 and not step == 0:
        #     # Calculate elapsed time in minutes.
        #     elapsed = format_time(time.time() - t0)
            
        #     # Report progress.
        #     print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))

        # Unpack this training batch from our dataloader and put to GPU. 
        # `batch` contains three pytorch tensors:
        #   [0]: input ids 
        #   [1]: attention masks
        #   [2]: labels 
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        #clear any previously calculated gradients before performing a backward pass.
        model.zero_grad()        

        # Perform a forward pass (evaluate the model on this training batch). calling `model` will in turn call the model's `forward` 
        result = model(b_input_ids, 
                       token_type_ids=None, 
                       attention_mask=b_input_mask, 
                       labels=b_labels,
                       return_dict=True)

        loss = result.loss # returns the classification/Softmax loss
        logits = result.logits # returns the classification scores before applying the Softmax activation.

        # Accumulate the training loss over all of the batches in an epoch
        #`loss` is a Tensor containing a single value; the `.item()` function just returns the Python value from the tensor.
        total_train_loss += loss.item()

        # Move logits and labels to CPU
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences, and
        # accumulate it over all batches.
        total_train_accuracy += flat_accuracy(logits, label_ids)

        # Perform a backward pass to calculate the gradients.
        loss.backward()

        # Clip the norm of the gradients to 1.0.
        # This is to help prevent the "exploding gradients" problem.
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # Update parameters and take a step using the computed gradient.
        optimizer.step()

        # Update the learning rate.
        scheduler.step()

    # Calculate the average train accuracy over all of the batches..
    avg_train_accuracy = total_train_accuracy / len(train_dataloader)
    # Calculate the average loss over all of the batches.
    avg_train_loss = total_train_loss / len(train_dataloader)            
    
    # Measure how long this epoch took.
    training_time = format_time(time.time() - t0)

    print("")
    print("  Average training Accuracy: {0:.2f}".format(avg_train_accuracy))
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epcoh took: {:}".format(training_time))
    return avg_train_accuracy, avg_train_loss, training_time

def evaluation(dataloader):
  # ========================================
    #               Validation
    # ========================================
    # After the completion of each training epoch, measure our performance on
    # our validation set.

    print("")
    print("Running Validation...")
    all_predictions = []
    targs = []

    t0 = time.time()

    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.
    model.eval()

    # Tracking variables 
    total_eval_accuracy = 0
    total_eval_loss = 0
    nb_eval_steps = 0

    # Evaluate data for one epoch
    for batch in dataloader:
        
        # Unpack this val batch from our dataloader and again put them to GPU. 
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)
        
        # Tell pytorch not to bother with constructing the compute graph during
        # the forward pass, since this is only needed for backprop (training).
        with torch.no_grad():        

            # Forward pass, calculate logit predictions.
            # token_type_ids is the same as the "segment ids", which 
            # differentiates sentence 1 and 2 in 2-sentence tasks.
            result = model(b_input_ids, 
                           token_type_ids=None, 
                           attention_mask=b_input_mask,
                           labels=b_labels,
                           return_dict=True)

        # Get the loss and "logits" output by the model. The "logits" are the 
        # output values prior to applying an activation function like the 
        # softmax.
        loss = result.loss
        logits = result.logits

        
            
        # Accumulate the validation loss.
        total_eval_loss += loss.item()

        # Move logits and labels to CPU
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # Calculate the accuracy for this batch of test sentences, and
        # accumulate it over all batches.
        total_eval_accuracy += flat_accuracy(logits, label_ids)

        rounded_predictions = np.argmax(logits, axis=1).flatten()
        all_predictions.extend(rounded_predictions)
        targs.extend(label_ids)
        

    # Report the final accuracy for this validation run.
    avg_val_accuracy = total_eval_accuracy / len(dataloader)
    

    # Calculate the average loss over all of the batches.
    avg_val_loss = total_eval_loss / len(dataloader)
    
    # Measure how long the validation run took.
    validation_time = format_time(time.time() - t0)
    
    print("  Validation Accuracy: {0:.2f}".format(avg_val_accuracy))
    print("  Validation Loss: {0:.2f}".format(avg_val_loss))
    print("  Validation took: {:}".format(validation_time))
    return avg_val_accuracy, avg_val_loss, validation_time, all_predictions, targs

import random
import numpy as np

# We'll store a number of quantities such as training and validation loss, 
# validation accuracy, and timings.
training_stats = []

# Measure the total training time for the whole run.
total_t0 = time.time()

# For each epoch...
for epoch_i in range(0, epochs):
    
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))

    avg_train_accuracy, avg_train_loss, training_time = train()
    # ------------end of Training------------#

    avg_val_accuracy, avg_val_loss, validation_time, all_pred, targets = evaluation(validation_dataloader)

    # Record all statistics from this epoch.
    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Training Accur.': avg_train_accuracy,
            'Valid. Loss': avg_val_loss,
            'Valid. Accur.': avg_val_accuracy,
            'Training Time': training_time,
            'Validation Time': validation_time
        }
    )

print("")
print("Training complete!")

print("Total training took {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))

#avg_val_accuracy, avg_val_loss, validation_time = evaluation()

"""Summary of training process"""

import pandas as pd

# Display floats with two decimal places.
pd.set_option('precision', 4)

# Create a DataFrame from our training statistics.
df_stats = pd.DataFrame(data=training_stats)

# Use the 'epoch' as the row index.
df_stats = df_stats.set_index('epoch')

# Display the table.
df_stats

"""Plot the Accuracy and loss"""

# Plot the dataframes
plt.plot(df_stats['Training Accur.'])
plt.plot(df_stats['Valid. Accur.'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'Val'], loc='upper left')
plt.xticks([1, 2, 3, 4])
plt.show()
plt.savefig(base_path +result_path+ 'plot_accuracy_'+ model_name + classification_class + Dataset+'.png')

plt.plot(df_stats['Training Loss'])
plt.plot(df_stats['Valid. Loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'Val'], loc='upper left')
plt.xticks([1, 2, 3, 4])
plt.show()
plt.savefig(base_path +result_path+ 'plot_loss_'+ model_name + classification_class + Dataset+'.png')

"""### Test the model, make predications, Show classification Report

We'll load up the parameters that gave us the best validation loss and try these on the test set - which gives us our best results so far!
"""

#model.load_state_dict(torch.load('/content/drive/MyDrive/Independent Study-NLP/Bengali_Sentiment-master/Dataset/Gsuit_'+ model_name + classification_class+ Dataset+'.pt'))

avg_test_accuracy, avg_test_loss, test_time, all_predictions, targets  = evaluation(test_dataloader) 

print(f'Test Loss: {avg_test_loss:.3f} | Test Acc: {avg_test_accuracy*100:.2f}%')

df_stats['Test Accur.']= avg_test_accuracy
df_stats['Test loss']= avg_test_loss
df_stats['num_classes']= model.num_labels
df_stats['Train_Size']= len(train_dataloader)/total_df.shape[0]
df_stats['Test_Size']= len(test_dataloader)/total_df.shape[0]
df_stats['Valid_Size']= len(validation_dataloader)/total_df.shape[0]
df_stats['Optimizer']= 'Adam'
df_stats['Learning_rate']= learning_rate
df_stats['Loss_function']= 'CrossEntropyLoss'
df_stats['Batch_Size']= batch_size
df_stats

df_stats.to_csv(base_path +result_path+ 'results_'+ model_name + classification_class + Dataset+'.csv')

from sklearn.metrics import classification_report

categories = ['Negative', 'Positive']

# y_pred_list = []
# y_test_target_list = []
# with torch.no_grad():
#   model.eval()
#   for batch in test_iterator:
#     y_test_pred = model(batch.text)
#     rounded_predictions = torch.max(y_test_pred, 1)[1]
#     # print("rounded_predictions: ",rounded_predictions)
#     y_pred_list.extend(rounded_predictions.cpu().detach().numpy())
#     y_test_target_list.extend(batch.labels.cpu().detach().numpy())

# from collections import Counter
# print(Counter(targets).keys()) # equals to list(set(words))
# print(Counter(targets).values())

report = classification_report(all_predictions, targets, target_names=categories, output_dict=True)
df_report = pd.DataFrame(report).transpose()
df_report

df_report.to_csv(base_path +report_path+'report_'+ model_name + classification_class + Dataset+'.csv')

